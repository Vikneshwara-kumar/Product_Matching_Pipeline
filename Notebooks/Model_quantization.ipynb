{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 62059,
     "status": "ok",
     "timestamp": 1741230911334,
     "user": {
      "displayName": "Food buddies",
      "userId": "12051795843088448432"
     },
     "user_tz": -60
    },
    "id": "i7Xq_NUT7KRe",
    "outputId": "55cb5d15-889a-478c-9f29-48b5008a03f3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 338M/338M [00:10<00:00, 32.5MiB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_tensorrt as torchtrt\n",
    "import clip\n",
    "\n",
    "# Set device to CUDA if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CLIP model (ViT-B/32) and its preprocessing pipeline\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the visual encoder from CLIP\n",
    "visual_encoder = model.visual.to(device)\n",
    "\n",
    "# Compile the visual encoder with Torch-TensorRT\n",
    "trt_visual_encoder = torchtrt.compile(\n",
    "    visual_encoder,\n",
    "    inputs=[torchtrt.Input((1, 3, 224, 224), dtype=torch.half)],\n",
    "    enabled_precisions={torchtrt.dtype.f16}  # use half-precision\n",
    ")\n",
    "\n",
    "# Create a traced TorchScript module for deployment\n",
    "ts_trt_visual_encoder = torch.jit.trace(\n",
    "    trt_visual_encoder,\n",
    "    torch.rand(1, 3, 224, 224).to(device).type(torch.half)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36982,
     "status": "ok",
     "timestamp": 1741231127251,
     "user": {
      "displayName": "Food buddies",
      "userId": "12051795843088448432"
     },
     "user_tz": -60
    },
    "id": "In8kIZOM_Bfg",
    "outputId": "c351c075-7258-4861-8235-291997197b42"
   },
   "outputs": [],
   "source": [
    "# Defining a wrapper for the text encoder.\n",
    "# This module will take tokenized text as input and return the text embeddings.\n",
    "class CLIPTextEncoder(torch.nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super(CLIPTextEncoder, self).__init__()\n",
    "        self.clip_model = clip_model\n",
    "    def forward(self, tokens):\n",
    "        # tokens: expected shape [batch, 77] with dtype torch.int64\n",
    "        return self.clip_model.encode_text(tokens)\n",
    "\n",
    "# Instantiate the text encoder wrapper.\n",
    "text_encoder = CLIPTextEncoder(model).to(device)\n",
    "text_encoder.eval()\n",
    "\n",
    "# Compile the text encoder with Torch-TensorRT, An input shape of (1, 77) (batch size 1 and 77 tokens) is specified.\n",
    "trt_text_encoder = torchtrt.compile(\n",
    "    text_encoder,\n",
    "    inputs=[torchtrt.Input((1, 77), dtype=torch.int64)],\n",
    "    enabled_precisions={torchtrt.dtype.f16}  # Enable FP16 kernels if supported.\n",
    ")\n",
    "\n",
    "# Create a traced TorchScript module for deployment.\n",
    "# Use a sample tokenized text input.\n",
    "sample_text = \"Hello, world!\"\n",
    "# Tokenize the sample text using CLIP's tokenize function.\n",
    "# clip.tokenize returns a tensor of shape [batch, 77] and type torch.int64.\n",
    "example_tokens = clip.tokenize([sample_text]).to(device).long()\n",
    "\n",
    "# Trace the compiled module.\n",
    "ts_trt_text_encoder = torch.jit.trace(\n",
    "    trt_text_encoder,\n",
    "    example_tokens  # Ensure the traced input has the same dtype and shape.\n",
    ")\n",
    "\n",
    "# Test inference: Pass in tokenized text and get embeddings.\n",
    "with torch.no_grad():\n",
    "    embeddings = ts_trt_text_encoder(example_tokens)\n",
    "    print(\"Text embeddings shape:\", embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3178,
     "status": "ok",
     "timestamp": 1741231232280,
     "user": {
      "displayName": "Food buddies",
      "userId": "12051795843088448432"
     },
     "user_tz": -60
    },
    "id": "2iK7Pa857N79"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(\"/content/model_repository/clip_visual/1\", exist_ok=True)\n",
    "# Save the model\n",
    "torch.jit.save(ts_trt_visual_encoder, \"/content/model_repository/clip_visual/1/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 17541,
     "status": "ok",
     "timestamp": 1741231251048,
     "user": {
      "displayName": "Food buddies",
      "userId": "12051795843088448432"
     },
     "user_tz": -60
    },
    "id": "RD0jr_A5_8jO"
   },
   "outputs": [],
   "source": [
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(\"/content/model_repository/clip_text/1\", exist_ok=True)\n",
    "# Save the model\n",
    "torch.jit.save(ts_trt_text_encoder, \"/content/model_repository/clip_text/1/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4171,
     "status": "ok",
     "timestamp": 1741231266797,
     "user": {
      "displayName": "Food buddies",
      "userId": "12051795843088448432"
     },
     "user_tz": -60
    },
    "id": "IoeH3QKl7WIo",
    "outputId": "413e8953-0744-47d4-b679-585e507a0767"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample image not found or failed to load. Using a random tensor for testing.\n",
      "Extracted visual features shape: torch.Size([1, 512])\n",
      "Predicted class index: 321\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Set device to CUDA if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the saved TorchScript model\n",
    "model_path = \"/content/model_repository/clip_visual/1/model.pt\"\n",
    "visual_encoder = torch.jit.load(model_path).to(device)\n",
    "visual_encoder.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Load CLIP to get its preprocessing pipeline\n",
    "_, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Attempt to load a sample image; if not available, use a random tensor\n",
    "try:\n",
    "    # Replace \"sample.jpg\" with the path to your test image if needed\n",
    "    image = Image.open(\"Adidas-1.jpg\").convert(\"RGB\")\n",
    "    # Preprocess image to get a tensor of shape (3, 224, 224)\n",
    "    image_tensor = preprocess(image)\n",
    "    # Convert to half-precision to match the compiled model's input dtype\n",
    "    image_tensor = image_tensor.half()\n",
    "except Exception as e:\n",
    "    print(\"Sample image not found or failed to load. Using a random tensor for testing.\")\n",
    "    image_tensor = torch.rand(3, 224, 224).half()\n",
    "\n",
    "# Add a batch dimension and move the tensor to the selected device\n",
    "input_tensor = image_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "# Perform inference with the loaded model\n",
    "with torch.no_grad():\n",
    "    features = visual_encoder(input_tensor)\n",
    "    output = visual_encoder(input_tensor)\n",
    "\n",
    "print(\"Extracted visual features shape:\", features.shape)\n",
    "\n",
    "# Get the predicted class index (largest logit)\n",
    "_, predicted = torch.max(output, 1)\n",
    "print(\"Predicted class index:\", predicted.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1239,
     "status": "ok",
     "timestamp": 1741231275177,
     "user": {
      "displayName": "Food buddies",
      "userId": "12051795843088448432"
     },
     "user_tz": -60
    },
    "id": "kZ7ezd5NAoKZ",
    "outputId": "d027f173-3313-4000-ea41-f0de8bc9a7b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading TorchScript module from /content/model_repository/clip_text/1/model.pt ...\n"
     ]
    }
   ],
   "source": [
    "# Set device to CUDA if available.\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Load the TorchScript text encoder module.\n",
    "model_path = \"/content/model_repository/clip_text/1/model.pt\"  # Ensure this file is in the current directory.\n",
    "print(f\"Loading TorchScript module from {model_path} ...\")\n",
    "ts_model = torch.jit.load(model_path, map_location=device)\n",
    "ts_model.eval()\n",
    "\n",
    "# Function to perform inference on input text.\n",
    "def infer_text(text: str):\n",
    "    # Tokenize the input text using CLIP's tokenizer.\n",
    "    # clip.tokenize returns a tensor of shape [batch, 77] with dtype torch.int64.\n",
    "    tokens = clip.tokenize([text]).to(device).long()  # Ensure tokens are Long tensors.\n",
    "\n",
    "    # Run inference using the traced model.\n",
    "    with torch.no_grad():\n",
    "        embeddings = ts_model(tokens)\n",
    "    return embeddings\n",
    "\n",
    "# Example inference: specify your text input.\n",
    "input_text = \"whats up chatgpt> how are you doing?\"\n",
    "embeddings = infer_text(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1741231278220,
     "user": {
      "displayName": "Food buddies",
      "userId": "12051795843088448432"
     },
     "user_tz": -60
    },
    "id": "qeuxoZf3DFZH",
    "outputId": "4bb4a05a-e7f7-487f-c3ba-e479e78d3dbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding as NumPy array: [[0.26123046875, 0.180908203125, -0.213134765625, -0.2049560546875, -0.0419921875, 0.105712890625, 0.28076171875, -0.27490234375, -0.036285400390625, 0.08984375, 0.079345703125, -0.1348876953125, 0.3369140625, -0.007724761962890625, -0.14892578125, 0.335693359375, -0.51953125, -0.2724609375, -0.306884765625, -0.301025390625, 0.194580078125, -0.6162109375, -0.2081298828125, 0.11334228515625, 0.015869140625, 0.139404296875, 0.14990234375, 0.03277587890625, -0.05047607421875, 0.4091796875, -0.1324462890625, -0.217529296875, -0.024078369140625, 0.021759033203125, -0.03076171875, -0.1419677734375, 0.106689453125, -0.042205810546875, 0.19140625, -0.157470703125, -0.09649658203125, 0.06756591796875, 0.271728515625, 0.00954437255859375, -0.031768798828125, 0.18798828125, -0.1923828125, -0.0562744140625, -0.07171630859375, 0.26708984375, -0.020355224609375, -0.31982421875, 0.155517578125, -0.0595703125, -0.4990234375, 0.1304931640625, -0.197509765625, 0.032806396484375, -0.3388671875, -0.2056884765625, -0.050506591796875, -0.25634765625, 0.004528045654296875, 0.6103515625, 0.006855010986328125, 0.239501953125, -0.330078125, 0.2440185546875, 0.4365234375, 0.1959228515625, -0.1300048828125, -0.004856109619140625, 0.47705078125, -0.185302734375, -0.1129150390625, -0.3642578125, -0.37451171875, -0.420654296875, -0.10906982421875, -0.11077880859375, 0.056243896484375, -0.10772705078125, 0.5634765625, -0.368408203125, -0.339599609375, 0.125, 0.1611328125, -0.045257568359375, 0.20263671875, -0.055267333984375, -0.1649169921875, 0.166259765625, -0.81787109375, 0.431640625, 0.2418212890625, -0.1832275390625, 0.1898193359375, -0.2222900390625, 0.12030029296875, -0.1285400390625, -0.630859375, -0.02484130859375, -0.30419921875, -0.04486083984375, -0.138671875, 0.341064453125, 0.08355712890625, -0.123779296875, -0.208740234375, 0.2423095703125, -0.0711669921875, 0.282470703125, 0.04754638671875, -0.34423828125, -0.010711669921875, 0.19287109375, 0.452392578125, -0.04443359375, 0.052886962890625, 0.1480712890625, 0.0567626953125, 0.1348876953125, -0.1387939453125, 0.1597900390625, 0.1944580078125, 0.62646484375, -0.0638427734375, 0.03643798828125, 0.27197265625, -0.22412109375, -0.4150390625, -0.0582275390625, -0.732421875, 1.5068359375, 0.017364501953125, 0.22119140625, 0.07421875, -0.390625, -0.03955078125, 0.15966796875, 0.0771484375, -0.257568359375, 0.023956298828125, -0.4130859375, -0.2313232421875, -0.04412841796875, 0.4462890625, 0.1890869140625, 0.2105712890625, 0.36767578125, 0.4208984375, 0.326171875, 0.293212890625, 0.0819091796875, 0.10260009765625, 0.1885986328125, 0.55908203125, -0.2147216796875, 0.373291015625, -0.042999267578125, 0.045989990234375, 0.474365234375, -0.06561279296875, 0.109619140625, 0.1651611328125, 0.046234130859375, 0.2822265625, -0.07720947265625, -0.521484375, -0.355224609375, -0.1502685546875, 0.393798828125, -0.056243896484375, 0.1905517578125, -0.31640625, 0.3330078125, 0.1690673828125, -0.31591796875, 0.01229095458984375, -0.1466064453125, -0.576171875, 0.09344482421875, 0.060638427734375, -0.2130126953125, -0.09124755859375, 0.1876220703125, -0.13427734375, -0.174072265625, 0.18408203125, 0.0010213851928710938, 0.349609375, 0.05194091796875, 0.1270751953125, 0.252197265625, 0.24560546875, 0.181884765625, 0.1922607421875, 0.2056884765625, 0.307861328125, 0.290771484375, 0.423828125, 0.42041015625, 0.09832763671875, 0.0655517578125, 0.019134521484375, 0.0149993896484375, 0.314697265625, 0.28759765625, 0.3740234375, 0.262451171875, -0.2098388671875, 0.06805419921875, -0.050689697265625, -0.282470703125, -0.482666015625, -0.058990478515625, -0.0980224609375, -0.08111572265625, -0.3427734375, -0.2978515625, 0.268798828125, -0.266357421875, -0.611328125, -0.4873046875, 0.034576416015625, 0.404296875, -0.0192108154296875, -0.5205078125, -0.4736328125, -0.2080078125, 0.08575439453125, -0.03277587890625, 0.2391357421875, 0.189208984375, 0.41357421875, -0.1300048828125, 0.095703125, 0.109619140625, -0.309326171875, 0.41162109375, -0.58984375, 0.431396484375, 0.033172607421875, 0.0197906494140625, -0.043853759765625, -0.075439453125, -0.1339111328125, -0.53515625, 0.446533203125, -0.0653076171875, 0.68408203125, 0.086181640625, 0.1876220703125, 0.11639404296875, 0.0263214111328125, -0.036956787109375, -0.00904083251953125, -0.22705078125, -0.0257415771484375, -0.125732421875, -0.24365234375, 0.01325225830078125, 0.279052734375, 0.2269287109375, -0.271728515625, 0.066650390625, 0.09814453125, 0.297119140625, -0.2349853515625, 0.1949462890625, 0.1475830078125, 0.2166748046875, -0.1072998046875, -0.533203125, -0.068359375, 0.322021484375, 0.0101470947265625, 0.261962890625, -0.4619140625, -0.267333984375, -0.1396484375, -0.240966796875, 0.6142578125, 0.2237548828125, -0.56298828125, -0.170166015625, 0.05645751953125, 0.349365234375, 0.515625, 0.04876708984375, -0.54345703125, 0.18408203125, 0.00374603271484375, -0.76904296875, 0.0245361328125, 0.1976318359375, -0.72265625, -0.353271484375, -0.2646484375, 0.116943359375, -0.4375, 0.5458984375, -0.22705078125, -0.03558349609375, -0.2005615234375, -0.436279296875, 0.07611083984375, 0.3759765625, 0.14208984375, 0.46240234375, -0.249755859375, -0.2489013671875, 1.51953125, -0.72900390625, 0.153564453125, 0.33251953125, -0.2342529296875, 0.289306640625, 0.2164306640625, -0.030120849609375, 0.285400390625, 0.048858642578125, 0.1966552734375, 0.1270751953125, -0.2418212890625, 0.1513671875, 0.11358642578125, 0.0239410400390625, -0.462646484375, 0.00464630126953125, -0.004016876220703125, -0.1260986328125, 0.107666015625, 0.428466796875, -0.0257415771484375, -0.323486328125, 0.16552734375, 0.059112548828125, 0.036773681640625, -0.18603515625, -0.2010498046875, -0.1474609375, -0.1412353515625, -0.59326171875, 0.0838623046875, 0.11175537109375, 0.230224609375, -0.4794921875, 0.463623046875, 0.406494140625, 0.2464599609375, -0.1212158203125, -0.404052734375, -0.044525146484375, 0.51171875, -0.0303192138671875, 0.3046875, 0.478515625, -0.131591796875, 0.0089569091796875, -0.218505859375, 0.20849609375, 0.323974609375, 0.05084228515625, -0.309326171875, -0.305908203125, -0.022064208984375, -0.052520751953125, 0.05535888671875, 0.1627197265625, 0.1304931640625, -0.31005859375, 0.251220703125, -0.07647705078125, -0.10760498046875, 0.1102294921875, 0.062347412109375, -0.52880859375, -0.1876220703125, 0.447998046875, -0.1962890625, 0.33447265625, -0.434814453125, -0.01212310791015625, 0.1904296875, -0.10675048828125, -0.271240234375, 0.1942138671875, 0.07794189453125, -0.0931396484375, -0.400390625, -0.298583984375, 0.2325439453125, 0.274169921875, -0.13720703125, 0.281005859375, 0.32275390625, 0.0557861328125, -0.349365234375, -0.399658203125, -0.127685546875, 0.154052734375, 0.1453857421875, -0.026763916015625, 0.479736328125, 0.301513671875, 0.044403076171875, -0.021240234375, 0.237548828125, 0.026611328125, 0.0294647216796875, -0.350341796875, -0.05633544921875, -0.24560546875, 0.10504150390625, 0.0035495758056640625, 0.0740966796875, -0.467529296875, -0.3515625, -0.0263519287109375, 0.0270233154296875, -0.37548828125, 0.1646728515625, 0.428466796875, -0.188232421875, 0.47900390625, -0.0804443359375, -0.2286376953125, -0.0069580078125, -0.2281494140625, -0.1221923828125, 0.1494140625, -0.2734375, -0.08856201171875, -0.16748046875, -0.336181640625, 0.116943359375, 0.04083251953125, 0.0594482421875, -0.24267578125, 0.069580078125, 0.1158447265625, 0.03912353515625, -0.5390625, 0.247314453125, 0.0689697265625, -0.1446533203125, -0.07965087890625, 0.142333984375, -0.25732421875, 0.1021728515625, -0.1595458984375, -0.07012939453125, -0.2115478515625, 0.07781982421875, 0.04302978515625, -0.1807861328125, 0.2646484375, 0.57763671875, 0.1336669921875, 0.0723876953125, -0.0645751953125, -0.52294921875, -0.320068359375, -0.131103515625, 0.288330078125, 0.08349609375, -0.111083984375, 0.07843017578125, -0.347900390625, -0.044586181640625, 0.269287109375, -0.383544921875, -0.07843017578125, -0.2386474609375, -0.185791015625, -0.02880859375, 0.25146484375, -0.2568359375, 0.06060791015625, 0.2822265625, 0.268310546875, 0.0187530517578125, -0.165283203125, -0.1773681640625, -0.1727294921875, -0.13916015625, -0.269287109375, 0.400634765625, -0.11474609375, 0.09466552734375, -0.1195068359375, 0.0838623046875, 0.66162109375, -0.392822265625, 0.0682373046875, -0.05010986328125, -0.4169921875, -0.48583984375, -0.231201171875, 0.4345703125, -0.235107421875, -0.1021728515625, 0.0165252685546875, -0.0985107421875, 0.08294677734375, 0.0028781890869140625, 0.28271484375, -0.342529296875, 0.3935546875, -0.55908203125, -0.253662109375]]\n"
     ]
    }
   ],
   "source": [
    "embeddings_np = embeddings.cpu().numpy().tolist()\n",
    "print(\"Embedding as NumPy array:\", embeddings_np)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMuz1XcPRs6+K7mq4OOBXih",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
