version: '3.8'
services:
  # 1. Streamlit Application
  app:
    build: .
    container_name: product-matching-app
    depends_on:
      - triton
      - qdrant
      - mongodb
    ports:
      - "8501:8501"
    environment:
      # These environment variables can be used by your Python code if needed
      - TRITON_URL=http://triton:8000
      - QDRANT_URL=http://qdrant:6333
      - MONGO_URI=mongodb://mongodb:27017
    volumes:
      # Optional: Mount the local code into the container for dev purposes
      - .:/app

  # 2. Triton Inference Server
  triton:
    image: nvcr.io/nvidia/tritonserver:25.02-py3
    container_name: triton-inference-server
    # If you have a GPU, uncomment the lines below:
    # runtime: nvidia
    # environment:
    #   - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      # Mount your Triton model repository
      - ./model/model_repository:/models
    command: tritonserver --model-repository=/models
    ports:
      - "8000:8000"  # HTTP
      - "8001:8001"  # gRPC
      - "8002:8002"  # Metrics

  # 3. Qdrant Vector Database
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    ports:
      - "6333:6333"  # REST API
      - "6334:6334"  # gRPC

  # 4. MongoDB
  mongodb:
    image: mongo:latest
    container_name: mongodb
    ports:
      - "27017:27017"
    volumes:
      - mongo_data:/data/db

volumes:
  mongo_data:
